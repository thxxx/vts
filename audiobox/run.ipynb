{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7040d603-d523-478e-803c-56235108f3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# default\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import concurrent\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import soundfile as sf\n",
    "import io\n",
    "# optional\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import repeat\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from vocos import get_voco\n",
    "from model.module import AudioBoxModule\n",
    "from torchode.interface import solve_ivp\n",
    "import torchaudio\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413d6670-a06f-40ac-93e7-a50f34ece7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voco type  :  oobleck\n"
     ]
    }
   ],
   "source": [
    "# model functions\n",
    "class Infer:\n",
    "    def __init__(self, path: Path):\n",
    "        self.device = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "        self.model = AudioBoxModule.load_from_checkpoint(path).to(self.device)\n",
    "        self.model.eval()\n",
    "        self.voco = get_voco(self.model.voco_type).to(self.device)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        self.steps = 64\n",
    "        self.alpha = 3.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    def encode_text(self, texts: list[str]) -> tuple[Tensor, Tensor]:\n",
    "        batch_encoding = self.tokenizer(\n",
    "            [text + self.tokenizer.eos_token for text in texts],\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=127,\n",
    "            truncation=\"longest_first\",\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        phoneme = batch_encoding.input_ids.to(self.device)\n",
    "        phoneme_mask = batch_encoding.attention_mask.to(self.device) > 0\n",
    "        phoneme_emb = self.model.t5(\n",
    "            input_ids=phoneme, attention_mask=phoneme_mask\n",
    "        ).last_hidden_state\n",
    "\n",
    "        return phoneme_emb, phoneme_mask\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    # @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    # def clap_rank(self, audios: Tensor, texts: list[str]) -> Tensor:\n",
    "    #     audios = audios[:, : self.clap_audio_len].mean(dim=-1)\n",
    "    #     audios = audios.float()\n",
    "    #     text_embed = self.clap.get_text_embeddings(texts)\n",
    "    #     audio_embed = self.clap.clap.audio_encoder(audios)[0]\n",
    "\n",
    "    #     similarity = F.cosine_similarity(text_embed, audio_embed)\n",
    "    #     args = torch.argsort(similarity, dim=0, descending=True)\n",
    "    #     return args\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @torch.autocast(device_type=\"cuda\")\n",
    "    def generate(\n",
    "        self, texts: list[str], dur: float, cfg=3.0\n",
    "    ) -> list[np.ndarray]:\n",
    "        phoneme_emb, phoneme_mask = self.encode_text(texts)\n",
    "        batch_size = phoneme_emb.shape[0]\n",
    "\n",
    "        target_len = round(self.model.sampling_rate * dur)\n",
    "        latent_len = self.voco.encode_length(target_len)\n",
    "        audio_mask = torch.ones(\n",
    "            batch_size, latent_len, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "        audio_context = torch.zeros(\n",
    "            batch_size, latent_len, self.voco.latent_dim, device=self.device\n",
    "        )\n",
    "\n",
    "        if latent_len < 192:\n",
    "            audio_mask = F.pad(audio_mask, (0, 192 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 192 - latent_len))\n",
    "        elif 192 < latent_len < 384:\n",
    "            audio_mask = F.pad(audio_mask, (0, 384 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 384 - latent_len))\n",
    "        elif 384 < latent_len < 768:\n",
    "            audio_mask = F.pad(audio_mask, (0, 768 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 768 - latent_len))\n",
    "        elif 768 < latent_len < 1536:\n",
    "            audio_mask = F.pad(audio_mask, (0, 1536 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 1536 - latent_len))\n",
    "\n",
    "        def fn(t: Tensor, y: Tensor):\n",
    "            out = self.model.audiobox.cfg(\n",
    "                w=y,\n",
    "                context=audio_context,\n",
    "                times=t,\n",
    "                alpha=cfg,\n",
    "                mask=audio_mask,\n",
    "                phoneme_emb=phoneme_emb,\n",
    "                phoneme_mask=phoneme_mask,\n",
    "            )\n",
    "            return out\n",
    "\n",
    "        y0 = torch.randn_like(audio_context)\n",
    "        t = torch.linspace(0, 1, self.steps, device=self.device)\n",
    "\n",
    "        t = repeat(t, \"n -> b n\", b=batch_size)\n",
    "        sol = solve_ivp(\n",
    "            fn,\n",
    "            # torch.compile(fn, dynamic=False),\n",
    "            y0,\n",
    "            t,\n",
    "            method_class=self.model.method, #self.model.torchode_method_klass,\n",
    "        )\n",
    "        sampled_audio = sol.ys[-1]\n",
    "\n",
    "        sample = self.voco.decode(sampled_audio)\n",
    "        sample = sample[:, :target_len]\n",
    "\n",
    "        sample = sample / sample.abs().amax(dim=1, keepdim=True).clamp_min(1)\n",
    "        # args = self.clap_rank(sample, texts)\n",
    "        # sample = sample[args]\n",
    "        # sample = sample[:cutoff]\n",
    "        sample = sample.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "        return [audio for audio in sample]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @torch.autocast(device_type=\"cuda\")\n",
    "    def variation(\n",
    "        self, audios: list[np.ndarray], texts: list[str], dur: float, corrupt: float, sr: list[int], cfg_score: int = 3.0\n",
    "    ) -> list[np.ndarray]:\n",
    "        phoneme_emb, phoneme_mask = self.encode_text(texts)\n",
    "        batch_size = phoneme_emb.shape[0]\n",
    "\n",
    "        audios = [audio / np.iinfo(audio.dtype).max for audio in audios]\n",
    "        audio_tensor = torch.from_numpy(np.stack(audios, axis=0)).to(self.device)\n",
    "        audio_tensor = audio_tensor.float()\n",
    "        ##\n",
    "        audio_tensor = audio_tensor.transpose(1, 2)\n",
    "        audio_tensor = torchaudio.functional.resample(audio_tensor, orig_freq=sr[0], new_freq=self.voco.sampling_rate)\n",
    "        audio_tensor = audio_tensor.transpose(1, 2)\n",
    "        if audio_tensor.shape[2] == 1:\n",
    "            audio_tensor = audio_tensor.repeat(1, 1, 2)\n",
    "        elif audio_tensor.shape[2] > 2:\n",
    "            audio_tensor = audio_tensor[:, :, :2]\n",
    "        target_len = audio_tensor.shape[1]\n",
    "        latent_len = self.voco.encode_length(target_len)\n",
    "        audio_enc = self.voco.encode(audio_tensor)\n",
    "        audio_mask = torch.ones(\n",
    "            batch_size, latent_len, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "        audio_context = torch.zeros(\n",
    "            batch_size, latent_len, self.voco.latent_dim, device=self.device\n",
    "        )\n",
    "\n",
    "        if latent_len < 192:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 192 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 192 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 192 - latent_len))\n",
    "        elif 192 < latent_len < 384:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 384 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 384 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 384 - latent_len))\n",
    "        elif 384 < latent_len < 768:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 768 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 768 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 768 - latent_len))\n",
    "        elif 768 < latent_len < 1536:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 1536 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 1536 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 1536 - latent_len))\n",
    "\n",
    "        sigma = 1e-3\n",
    "        c = 1.0 - corrupt\n",
    "        noised_enc = (audio_enc * c) + torch.randn_like(audio_enc) * (1 - (1 - sigma) * c)\n",
    "        corrupt_tensor = torch.tensor(1 - corrupt).to(self.device)\n",
    "        # print(\"corrupt_tensor : \", corrupt_tensor)\n",
    "\n",
    "        def forward(t: Tensor, y: Tensor):\n",
    "            # print(\"times : \", t)\n",
    "            out = self.model.audiobox.cfg(\n",
    "                w=y,\n",
    "                context=audio_context,\n",
    "                # times=t,\n",
    "                times=t,\n",
    "                alpha=cfg_score,\n",
    "                mask=audio_mask,\n",
    "                phoneme_emb=phoneme_emb,\n",
    "                phoneme_mask=phoneme_mask,\n",
    "            )\n",
    "            return out\n",
    "\n",
    "        # t = torch.linspace(c, 1, 64, device=self.device)\n",
    "        t = torch.linspace(0, corrupt, self.steps, device=self.device)\n",
    "\n",
    "        t = repeat(t, \"n -> b n\", b=batch_size)\n",
    "        sol = solve_ivp(\n",
    "            # torch.compile(forward, dynamic=False),\n",
    "            forward,\n",
    "            noised_enc,\n",
    "            t+corrupt_tensor, # 0.6 ~ 1.0\n",
    "            method_class=self.model.method #.torchode_method_klass,\n",
    "        )\n",
    "        sampled_audio = sol.ys[-1]\n",
    "\n",
    "        sample = self.voco.decode(sampled_audio)\n",
    "        new_target_len = round(self.model.sampling_rate * dur)\n",
    "        sample = sample[:, :new_target_len]\n",
    "\n",
    "        sample = sample / sample.abs().amax(dim=1, keepdim=True).clamp_min(1)\n",
    "        sample = sample.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "        return [audio for audio in sample]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    @torch.autocast(device_type=\"cuda\")\n",
    "    def variation_negative(\n",
    "        self, audios: list[np.ndarray], texts: list[str], negative_texts:list[str], dur: float, corrupt: float, sr: list[int], cfg_score: int = 3.0, nalpha: int = 1.0\n",
    "    ) -> list[np.ndarray]:\n",
    "        phoneme_emb, phoneme_mask = self.encode_text(texts)\n",
    "        negative_phoneme_emb, negative_phoneme_mask = self.encode_text(negative_texts)\n",
    "        batch_size = phoneme_emb.shape[0]\n",
    "\n",
    "        audios = [audio / np.iinfo(audio.dtype).max for audio in audios]\n",
    "        audio_tensor = torch.from_numpy(np.stack(audios, axis=0)).to(self.device)\n",
    "        audio_tensor = audio_tensor.float()\n",
    "        ##\n",
    "        audio_tensor = audio_tensor.transpose(1, 2)\n",
    "        audio_tensor = torchaudio.functional.resample(audio_tensor, orig_freq=sr[0], new_freq=self.voco.sampling_rate)\n",
    "        audio_tensor = audio_tensor.transpose(1, 2)\n",
    "        if audio_tensor.shape[2] == 1:\n",
    "            audio_tensor = audio_tensor.repeat(1, 1, 2)\n",
    "        elif audio_tensor.shape[2] > 2:\n",
    "            audio_tensor = audio_tensor[:, :, :2]\n",
    "        target_len = audio_tensor.shape[1]\n",
    "        latent_len = self.voco.encode_length(target_len)\n",
    "        audio_enc = self.voco.encode(audio_tensor)\n",
    "        audio_mask = torch.ones(\n",
    "            batch_size, latent_len, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "        audio_context = torch.zeros(\n",
    "            batch_size, latent_len, self.voco.latent_dim, device=self.device\n",
    "        )\n",
    "\n",
    "        if latent_len < 192:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 192 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 192 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 192 - latent_len))\n",
    "        elif 192 < latent_len < 384:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 384 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 384 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 384 - latent_len))\n",
    "        elif 384 < latent_len < 768:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 768 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 768 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 768 - latent_len))\n",
    "        elif 768 < latent_len < 1536:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 1536 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 1536 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 1536 - latent_len))\n",
    "\n",
    "        sigma = 1e-3\n",
    "        c = 1.0 - corrupt\n",
    "        noised_enc = (audio_enc * c) + torch.randn_like(audio_enc) * (1 - (1 - sigma) * c)\n",
    "        corrupt_tensor = torch.tensor(1 - corrupt).to(self.device)\n",
    "        # print(\"corrupt_tensor : \", corrupt_tensor)\n",
    "\n",
    "        def forward(t: Tensor, y: Tensor):\n",
    "            # print(\"times : \", t)\n",
    "            out = self.model.audiobox.cfg_negative(\n",
    "                w=y,\n",
    "                context=audio_context,\n",
    "                # times=t,\n",
    "                times=t,\n",
    "                alpha=cfg_score,\n",
    "                mask=audio_mask,\n",
    "                phoneme_emb=phoneme_emb,\n",
    "                phoneme_mask=phoneme_mask,\n",
    "                negative_phoneme_emb=negative_phoneme_emb,\n",
    "                negative_phoneme_mask=negative_phoneme_mask,\n",
    "                nalpha=nalpha\n",
    "            )\n",
    "            return out\n",
    "\n",
    "        # t = torch.linspace(c, 1, 64, device=self.device)\n",
    "        t = torch.linspace(0, corrupt, self.steps, device=self.device)\n",
    "\n",
    "        t = repeat(t, \"n -> b n\", b=batch_size)\n",
    "        sol = solve_ivp(\n",
    "            # torch.compile(forward, dynamic=False),\n",
    "            forward,\n",
    "            noised_enc,\n",
    "            t+corrupt_tensor, # 0.6 ~ 1.0\n",
    "            method_class=self.model.method #.torchode_method_klass,\n",
    "        )\n",
    "        sampled_audio = sol.ys[-1]\n",
    "\n",
    "        sample = self.voco.decode(sampled_audio)\n",
    "        new_target_len = round(self.model.sampling_rate * dur)\n",
    "        sample = sample[:, :new_target_len]\n",
    "\n",
    "        sample = sample / sample.abs().amax(dim=1, keepdim=True).clamp_min(1)\n",
    "        sample = sample.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "        return [audio for audio in sample]\n",
    "\n",
    "def download_file(url, filename):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Open file in binary write mode and save the content to the file\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "\n",
    "def remove_non_ascii(s):\n",
    "    return ''.join(i for i in s if ord(i)<128)\n",
    "\n",
    "def delete_audio_files():\n",
    "    files = glob.glob(os.path.join(\"audiobox/temp_audio_folder/\", '*'))\n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file)  # 파일 삭제\n",
    "            # print(f\"Deleted: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file}: {e}\")\n",
    "\n",
    "def download_audio_as_array(url):\n",
    "    \"\"\"\n",
    "    주어진 URL에서 오디오 파일을 다운로드하여 넘파이 배열과 샘플링 레이트를 반환하는 함수.\n",
    "    \"\"\"\n",
    "    # URL에서 바이너리 데이터 가져오기\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # 요청 실패 시 예외 발생\n",
    "\n",
    "    # BytesIO로 감싸서 soundfile로 읽기\n",
    "    data, samplerate = sf.read(io.BytesIO(response.content), always_2d=True)\n",
    "    return data, samplerate\n",
    "\n",
    "def convert_to_int16(audio_array):\n",
    "    \"\"\"\n",
    "    오디오 배열을 int16 형식으로 변환.\n",
    "    \"\"\"\n",
    "    # float형 오디오 배열을 int16 범위로 스케일링\n",
    "    audio_array = np.clip(audio_array, -1.0, 1.0)  # -1.0 ~ 1.0 범위로 제한\n",
    "    audio_int16 = (audio_array * 32767).astype(np.int16)\n",
    "    return audio_int16\n",
    "\n",
    "def process_audio_urls(url_list):\n",
    "    \"\"\"\n",
    "    URL 리스트를 받아 int16 타입의 오디오 배열로 변환.\n",
    "    반환: [n, length, channels] 형태의 3D 배열\n",
    "    \"\"\"\n",
    "    audio_arrays = []\n",
    "    samplerates = []\n",
    "    max_length = 0\n",
    "\n",
    "    # 각 URL에서 오디오 다운로드 및 변환\n",
    "    for url in url_list:\n",
    "        data, samplerate = download_audio_as_array(url)\n",
    "\n",
    "        samplerates.append(samplerate)  \n",
    "        data_int16 = convert_to_int16(data)\n",
    "        \n",
    "        # 길이 업데이트\n",
    "        max_length = max(max_length, data_int16.shape[0])\n",
    "        audio_arrays.append(data_int16)\n",
    "\n",
    "    # 모든 오디오 데이터를 동일한 길이로 패딩\n",
    "    padded_audios = []\n",
    "    for audio in audio_arrays:\n",
    "        padding = ((0, max_length - audio.shape[0]), (0, 0))  # 시간축 패딩 추가\n",
    "        padded_audio = np.pad(audio, padding, mode='constant', constant_values=0)\n",
    "        padded_audios.append(padded_audio)\n",
    "\n",
    "    # [n, length, channels] 형태의 3D 배열로 병합\n",
    "    result = np.stack(padded_audios, axis=0)\n",
    "    return result, samplerates\n",
    "\n",
    "#runpod handler\n",
    "def handler(event):\n",
    "    it=time.time()\n",
    "    # handle input data\n",
    "    input_data=event['input']\n",
    "    texts = input_data['descriptions'][0]\n",
    "    duration = input_data['duration']\n",
    "    file_paths = input_data['original_download_urls']\n",
    "    temperature = input_data['temperature']\n",
    "    \n",
    "    # generate\n",
    "    if file_paths == None:\n",
    "        output_audios = infer.generate([texts] * 5, duration)\n",
    "    else:\n",
    "        merged_audios, sr = process_audio_urls(file_paths)\n",
    "        # print(merged_audios[0])\n",
    "        # model inference\n",
    "        output_audios = infer.variation(merged_audios, [texts] * 5, duration, temperature, sr)\n",
    "    mt = time.time()\n",
    "    \n",
    "    #upload them\n",
    "    output_urls = [None for _ in range(len(output_audios))]\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n",
    "        upload_futures = {executor.submit(upload_audio, audio): idx for idx, audio in enumerate(output_audios)}\n",
    "        for future in concurrent.futures.as_completed(upload_futures):\n",
    "            idx = upload_futures[future]\n",
    "            try:\n",
    "                res = future.result()\n",
    "                output_urls[idx] = res\n",
    "            except Exception as error:\n",
    "                print('error:', error)\n",
    "                \n",
    "    # delete audios saved in the folder\n",
    "    delete_audio_files()\n",
    "    \n",
    "    #prepare the API response.\n",
    "    response_data={\n",
    "        \"output_download_urls\": output_urls\n",
    "    }\n",
    "    ft=time.time()\n",
    "    print(\"until generation time: \", mt-it)\n",
    "    print(\"total time: \",ft-it)\n",
    "    return json.dumps(response_data)\n",
    "\n",
    "# prepare model\n",
    "infer = Infer(Path(\"./new-stage-2.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ccc2bc6-b501-407d-8a6c-bc18eb9dfb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "save_file(infer.model.audiobox.state_dict(), \"audiobox0404.safetensors\")  # ← 진짜 safetensors 포맷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e02e3-30be-4a5b-8150-950faee1a736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50affff0-fcb1-4891-a59e-d165d321c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from audiotools import AudioSignal\n",
    "import time\n",
    "import torchaudio\n",
    "\n",
    "st = time.time()\n",
    "output_audios = infer.generate([\n",
    "    \"samurai man short and impactful shouting\"\n",
    "]*5, 10.0, 3.0)\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fa8db-5481-4598-979a-fd63bf3ced8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from audiotools import AudioSignal\n",
    "import time\n",
    "import torchaudio\n",
    "\n",
    "st = time.time()\n",
    "output_audios = infer.generate([\n",
    "    \"samurai man short and impactful shouting\"\n",
    "]*5, 3.0, 3.0)\n",
    "print(time.time() - st)\n",
    "\n",
    "st = time.time()\n",
    "output_audios = infer.generate([\n",
    "    \"samurai man short and impactful shouting\"\n",
    "]*5, 10.0, 3.0)\n",
    "print(time.time() - st)\n",
    "\n",
    "st = time.time()\n",
    "output_audios = infer.generate([\n",
    "    \"samurai man short and impactful shouting\"\n",
    "]*5, 20.0, 3.0)\n",
    "print(time.time() - st)\n",
    "\n",
    "st = time.time()\n",
    "output_audios = infer.generate([\n",
    "    \"samurai man short and impactful shouting\"\n",
    "]*5, 30.0, 3.0)\n",
    "print(time.time() - st)\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "output_audios = infer.generate([\n",
    "    \"samurai man short and impactful shouting\"\n",
    "]*5, 45.0, 3.0)\n",
    "print(time.time() - st)\n",
    "\n",
    "st = time.time()\n",
    "output_audios = infer.generate([\n",
    "    \"samurai man short and impactful shouting\"\n",
    "]*5, 60.0, 3.0)\n",
    "print(time.time() - st)\n",
    "\n",
    "# for oa in output_audios:\n",
    "#     audio = rearrange(oa, \"n c -> c n\")\n",
    "#     AudioSignal(audio, sample_rate=44100).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710154a7-a67d-4132-a210-4ed8789733e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524cb328-64ce-485d-8031-2d185b3b9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, sr = process_audio_urls([\"https://hpxjdveijpuehyuykkos.supabase.co/storage/v1/object/public/user_uploads/2025-04-01/788d9cf9-e013-4b39-8034-edae36b268f9/c106633b-9878-4855-8179-ef8445d5e6c1_epidemic-audios_misc-musical_Bl8QyruCWW.wav\"])\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ba736-a1a4-4ee0-818f-ab8263c1fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "output_audios = infer.generate([\n",
    "    \"Sound of scifi weapon charging and shooting.\"\n",
    "]*5, 3.0, 3.0) # bs, 44100*duration, stereo\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3886f-b575-43d5-b0de-f160dfb2d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSignal(rearrange(output_audios[1], 't s -> s t'), sample_rate=44100).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6c66f-1b74-4711-8df5-c923bb890469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msclap import CLAP\n",
    "\n",
    "# Load model (Choose version 'clapcap')\n",
    "clap_model = CLAP(version = 'clapcap', use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26728bf4-33c6-4456-a96c-c2dd19ef36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = clap_model.generate_caption([\"/workspace/2f613a75-02b1-43b7-b2ff-e715e62ecbdf.wav\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66fbb0-e931-484b-8254-6aa2465206af",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985e68c-e819-47f4-889c-0c0f64927d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df40a81-a195-4749-b114-2f1c8f30365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "audio, sr = librosa.load(\"/workspace/2f613a75-02b1-43b7-b2ff-e715e62ecbdf.wav\", sr=44100, mono=False)\n",
    "print(audio.shape)\n",
    "merged_audios = convert_to_int16(np.array(rearrange(audio[0], 't -> () t ()')))\n",
    "\n",
    "duration = 1.5\n",
    "temperature = 0.5\n",
    "texts = 'A gun is being fired several times.'\n",
    "AudioSignal(audio, sample_rate=44100).widget()\n",
    "\n",
    "for _ in range(3):\n",
    "    gen = infer.variation(merged_audios, [texts] * 1, duration, temperature, [sr], cfg_score=3.0)\n",
    "    AudioSignal(rearrange(gen[0], 't s -> s t'), sample_rate=44100).widget()\n",
    "\n",
    "print('\\n=======\\n')\n",
    "for _ in range(3):\n",
    "    gen = infer.variation(merged_audios, [''] * 1, duration, temperature, [sr], cfg_score=0.0)\n",
    "    AudioSignal(rearrange(gen[0], 't s -> s t'), sample_rate=44100).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2090bc-a217-4b81-adb7-0431ad017da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78f06a-7dce-40c6-8e9f-bbcdbf25f450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2581305-5fc8-4be3-b3ea-572e7a32233f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bbdf3-531a-4f67-b6f9-149eb319fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "from einops import rearrange\n",
    "\n",
    "voice_sets = [\n",
    "    {\n",
    "        \"ap\": \"./voice_samples/wtf2.wav\",\n",
    "        'prompt': \"Sound of dog barking.\",\n",
    "        'duration': 3.5\n",
    "    },\n",
    "    {\n",
    "        \"ap\": \"./voice_samples/charging.m4a\",\n",
    "        'prompt': \"Sound of scifi weapon charging and shooting.\",\n",
    "        'duration': 4.4\n",
    "    },\n",
    "    {\n",
    "        \"ap\": \"./voice_samples/wings.m4a\",\n",
    "        'prompt': \"Sound of huge eagle flapping wings.\",\n",
    "        'duration': 4.9\n",
    "    },\n",
    "]\n",
    "\n",
    "for d in voice_sets:\n",
    "    ap, prompt, duration = d[\"ap\"], d[\"prompt\"], d[\"duration\"]\n",
    "    audio, sr = torchaudio.load(ap)\n",
    "    print(audio.shape)\n",
    "    merged_audios = convert_to_int16(np.array(rearrange(audio[0], 't -> () t ()')))\n",
    "    \n",
    "    duration = duration\n",
    "    temperature = 0.6\n",
    "    texts = prompt\n",
    "    \n",
    "    gen = infer.variation_negative(merged_audios, [texts] * 1, ['Sound of male voice.'] * 1, duration, temperature, [sr], cfg_score=3.0, nalpha=0.0)\n",
    "    gen = rearrange(gen[0], 't s -> s t')\n",
    "    AudioSignal(audio, sample_rate=44100).widget()\n",
    "    AudioSignal(gen, sample_rate=44100).widget()\n",
    "\n",
    "    print(gen.shape)\n",
    "    merged_audios = convert_to_int16(np.array(rearrange(gen[0], 't -> () t ()')))\n",
    "    print(merged_audios.shape)\n",
    "    gen = infer.variation_negative(merged_audios, [texts] * 1, ['Sound of male voice.'] * 1, duration, temperature, [sr], cfg_score=3.0, nalpha=0.0)\n",
    "    gen = rearrange(gen[0], 't s -> s t')\n",
    "    AudioSignal(gen, sample_rate=44100).widget()\n",
    "\n",
    "    merged_audios = convert_to_int16(np.array(rearrange(gen[0], 't -> () t ()')))\n",
    "    print(merged_audios.shape)\n",
    "    gen = infer.variation_negative(merged_audios, [texts] * 1, ['Sound of male voice.'] * 1, duration, temperature, [sr], cfg_score=3.0, nalpha=0.0)\n",
    "    gen = rearrange(gen[0], 't s -> s t')\n",
    "    AudioSignal(gen, sample_rate=44100).widget()\n",
    "    print(\"\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a1857-dad7-43ab-a669-b44a91b80e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f44cb-ede8-427a-81d0-12e2f4bdb57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a161af6-2136-4b09-a5db-3ad39705413c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30740393-96b4-4a65-a633-da32853a8017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701dfb08-2a50-4c2c-aa19-d6002d273900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from audiotools import AudioSignal\n",
    "import time\n",
    "\n",
    "for prompt in prompts[20: 50]:\n",
    "    genp = [prompt]*8\n",
    "    if prompt[-1] == \".\":\n",
    "        genp[1] = genp[1].strip(\".\")\n",
    "        genp[2] = genp[2].strip(\".\")\n",
    "    else:\n",
    "        genp[1] = genp[1] + \".\"\n",
    "        genp[2] = genp[2] + \".\"\n",
    "    genp[3] = genp[3].lower()\n",
    "    genp[4] = genp[4].lower()\n",
    "\n",
    "    genp[5] = genp[5][0].upper() + genp[5][1:]\n",
    "    genp[6] = genp[6][0].upper() + genp[6][1:]\n",
    "    \n",
    "    output_audios = infer.generate(genp, 3.0, 3.0)\n",
    "    oa = torch.tensor(output_audios)\n",
    "    oa = rearrange(oa, \"b n c -> b c n\")\n",
    "    for idx, audio in enumerate(oa):\n",
    "        torchaudio.save(f'./outputs/{prompt}_{idx}.wav', audio, sample_rate=44100)\n",
    "    print(\"\\n\\n----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4e140-bf99-4aa9-80f1-1bfe5bedd4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829786b-d37c-4767-b402-8467883c4651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bccf31-cbb8-475b-8822-c5c0c142cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msclap import CLAP\n",
    "import torch\n",
    "# pip install git+https://github.com/microsoft/CLAP.git\n",
    "clap = CLAP(version=\"2023\", use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec95272-9c1f-4083-a036-42215c76ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "def clap_rank(audios: Tensor, texts: list[str]) -> Tensor:\n",
    "    audios = audios[:, 0, :44100*10]\n",
    "    print(audios.shape)\n",
    "    audios = audios.float()\n",
    "    text_embed = clap.get_text_embeddings(texts)\n",
    "    audio_embed = clap.clap.audio_encoder(audios)[0]\n",
    "\n",
    "    similarity = F.cosine_similarity(text_embed, audio_embed)\n",
    "    args = torch.argsort(similarity, dim=0, descending=False)\n",
    "    return args, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6542d55c-f61a-4e2f-8e0b-e0319e272b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from audiotools import AudioSignal\n",
    "import time\n",
    "\n",
    "for prompt in prompts[40:50]:\n",
    "    genp = [prompt]*8\n",
    "    if prompt[-1] == \".\":\n",
    "        genp[1] = genp[1].strip(\".\")\n",
    "        genp[2] = genp[2].strip(\".\")\n",
    "    else:\n",
    "        genp[1] = genp[1] + \".\"\n",
    "        genp[2] = genp[2] + \".\"\n",
    "    genp[3] = genp[3].lower()\n",
    "    genp[4] = genp[4].lower()\n",
    "\n",
    "    genp[5] = genp[5][0].upper() + genp[5][1:]\n",
    "    genp[6] = genp[6][0].upper() + genp[6][1:]\n",
    "    \n",
    "    output_audios = infer.generate(genp, 3.0, 3.0)\n",
    "    \n",
    "    oa = torch.tensor(output_audios)\n",
    "    oa = rearrange(oa, \"b n c -> b c n\")\n",
    "    \n",
    "    args, sims = clap_rank(oa.to('cuda'), [prompt]*8)\n",
    "    print(\"prompt : \", prompt)\n",
    "    print(\"All similarities : \", sims)\n",
    "    \n",
    "    for idx in [0, 1, 6, 7]:\n",
    "        print(f\"idx - {idx}, similarity : \", sims[args[idx]].item())\n",
    "        AudioSignal(oa[args[idx]], sample_rate=44100).widget()\n",
    "    print(\"\\n\\n----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21d988-9bb3-46da-991a-c006ce2bf7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99edf0-86e7-460a-848d-b615514a9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import CLAP.src.laion_clap as laion_clap\n",
    "\n",
    "def int16_to_float32(x):\n",
    "    return (x / 32767.0).astype(np.float32)\n",
    "\n",
    "def float32_to_int16(x):\n",
    "    x = np.clip(x, a_min=-1., a_max=1.)\n",
    "    return (x * 32767.).astype(np.int16)\n",
    "\n",
    "# model = laion_clap.CLAP_Module(enable_fusion=False)\n",
    "# model.to('cuda')\n",
    "# model.load_ckpt(model_id=1) # best model_id depends on what kind of data we use. 1 is good for short sound (not music, speech)\n",
    "\n",
    "clmodel = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7e941-02b6-4a26-8abc-88c8b6d7219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "state_dict = torch.load(\"/workspace/clap_final_0520_augmentation_epoch_best_29.pth\")\n",
    "\n",
    "# 불필요한 prefix 제거\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"_orig_mod.\"):\n",
    "        new_key = k.replace(\"_orig_mod.\", \"\")\n",
    "    else:\n",
    "        new_key = k\n",
    "    new_state_dict[new_key] = v\n",
    "\n",
    "# 모델에 로드\n",
    "clmodel.load_state_dict(new_state_dict)\n",
    "clmodel.eval()\n",
    "print(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28a8ca-f68f-4ee0-a212-d7797c02cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "def cosine_sim(vector1, vector2):\n",
    "    similarity = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))\n",
    "    return similarity[0, 0]\n",
    "\n",
    "for prompt in prompts[30:50]:\n",
    "    text_embed = clmodel.get_text_embedding(\n",
    "        [prompt]\n",
    "    )\n",
    "    \n",
    "    all_embeds = []\n",
    "    for idx in range(8):\n",
    "        all_embeds.append(torchaudio.load(f'/workspace/alignment-v3/audiobox/outputs/{prompt}_{idx}.wav')[0][0])\n",
    "\n",
    "    audios = []\n",
    "    for idx, audio_array in tqdm(enumerate(all_embeds)):\n",
    "        audio_embed = clmodel.get_audio_embedding_from_data(x = [np.array(audio_array)])\n",
    "        cod = cosine_sim(audio_embed, text_embed[0])\n",
    "        audios.append([audio_array, cod])\n",
    "    # 높으면 좋음.\n",
    "    print(\"prompt : \", prompt)\n",
    "    print(\"total scores - \", [a[1] for a in audios])\n",
    "    for ch in [7, 6, 1, 0]:\n",
    "        print(\"score - \", sorted(audios, key=lambda x: x[1])[ch][1])\n",
    "        AudioSignal(sorted(audios, key=lambda x: x[1])[ch][0], sample_rate=44100).widget()\n",
    "    print(\"\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cd2a6-9b0e-4fa9-bde2-fa42e3401d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63baa5f-ebf3-4666-8a82-8a308af8947f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7054d30-24b4-471e-8200-e6e8b2c6e89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce1469-ad4d-4cf3-8141-f76a533bb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_audios = infer.generate([\"a dog barking\"] * 5, 2.0)\n",
    "\n",
    "# original numpy==2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501dffe7-54c7-4dc8-83c2-2cda903315ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "print(output_audios[0].shape)\n",
    "\n",
    "audio = rearrange(output_audios[0], \"n c -> c n\")\n",
    "\n",
    "AudioSignal(audio, sample_rate=44100).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112f284-3ea1-4ae0-9ca3-adc464e06371",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_tensor = 0.7\n",
    "\n",
    "print(torch.linspace(1-corrupt_tensor, 1, 64))\n",
    "print(\"\\n\\n\")\n",
    "print(torch.linspace(0, corrupt_tensor, 64) + 1 - corrupt_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757fc11e-ff11-4a1a-be5a-ada2009e2ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4488d7-d039-4b7d-8da2-d86b40b51757",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Chirping sounds of a young bird, expressing its early calls.',\n",
    "     'The sound of keys jingling softly.',\n",
    "     'Sailors cheer and shout encouragement to one another during a game.',\n",
    "     \"The British LNER Steam Locomotive 'Flying Scotsman' whistles and gradually leaves a train station, pulling its coaches.\",\n",
    "     'At first the sound is calm, then Sharp electric surge on impact fast current discharge',\n",
    "     'quiet calm man gagged by kidnapper',\n",
    "     'Sounds of machinery and robotic movements in a futuristic facility.',\n",
    "     'Sounds of a tight grip on a wrist with accompanying fabric rustling and a subtle gripping noise.',\n",
    "     'The sound of an epic fiery flame strike exploding on the ground in a 2d strategy cartoony game',\n",
    "     'quiet calm man gagged by kidnapper',\n",
    "     'Sound of an explosion echoing in the distance.',\n",
    "     'Cinematic soundtrack reflecting a calm revelation in the movie.',\n",
    "     'the sound of a rogue fading into stealth mode in a 2d strategy cartoony game, smooth, shadow',\n",
    "     'Various cat sounds.',\n",
    "     'Whoosh of a sharp and thick sword slicing through the air, with magical spell',\n",
    "     'The sound of the last drop of milk cream being squeezed out of the box',\n",
    "     'Electrical ANOMALY smooth  passing impulse fly on circle',\n",
    "     'Nervous breathing of a man accompanied by visible bodily tremors.',\n",
    "     'At first the sound is calm, then Sharp electric surge on impact fast current discharge',\n",
    "     'Sounds from the Large Hadron Collider, capturing the essence of a futuristic game.',\n",
    "     '16-bit explosion sound effect suitable for gaming.',\n",
    "     'Realistic sound of a cardboard box softly thudding onto a surface, accompanied by subtle rustling and a dry, papery resonance.',\n",
    "     'Sound of a spaceship door opening.',\n",
    "     'Loud blast of a single-tone British diesel horn playing an E flat note.',\n",
    "     'Sound of the Void Eye shattering: a mix of breaking glass and releasing energy.',\n",
    "     'Subtle clicking sound of a wheel, designed for user interface interactions.',\n",
    "     'Sound of a bomb being thrown, creating a sizzling effect.',\n",
    "     'Sound of a magical portal opening, suitable for a game setting.',\n",
    "     'Sounds of liquid flow and bleeding effects.',\n",
    "     'The activation of an old analog switch produces a sharp mechanical click, followed by a high-pitched electronic beep, as a CRT screen flickers to life with soft static and a faint digital hum.',\n",
    "     'Subtle metallic click sounds from a quiet menu button in a user interface.',\n",
    "     'one',\n",
    "     'underwater word game ui button click',\n",
    "     'Sci-fi sound of a clunky laptop being flipped open.',\n",
    "     'The sound of leaves rustling in the wind.',\n",
    "     'Sounds of a mechanical device: gears and pins rotating and unlocking smoothly.',\n",
    "     'Sound of paper rustling as notes are read, suitable for a game setting.',\n",
    "     'The sound of a door closing vigorously with a loud bang',\n",
    "     'Heavy metal pin dragging and scraping sounds.',\n",
    "     'Horror soundscape of larvae emerging from the ground. A deep sound from beneath the ground',\n",
    "     'Sound of a rhino in its natural habitat, highlighting its movements and presence.',\n",
    "     'British Diesel Locomotive Class 47 idling sounds.',\n",
    "     'Sound of a heavy rock being tossed.',\n",
    "     'light interface denied sound'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670bc4d9-7ccc-460c-89b6-b3769f65632d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce75e9c-9116-4f0b-bdc5-c18a4f9c1735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a01a9-b83f-4360-b8e0-3ae5e7954478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d458c06-545f-4307-b890-e257d11d54e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer.model.max_audio_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75985377-1d2d-45aa-a775-86046270a751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477bca4-eb64-4e93-9012-2a0c524b3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"new-stage-2.ckpt\", map_location=\"cpu\")\n",
    "\n",
    "print(ckpt.keys())\n",
    "# 출력 예시: dict_keys(['state_dict', 'hyper_parameters', 'optimizer_states', 'lr_schedulers', ...])\n",
    "\n",
    "print(ckpt['hyper_parameters'])  # config 내용이 여기 있을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d57546-ddea-48b9-81a6-91c683dd045a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61131f-940f-459b-bdc7-e953f524e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio_as_array(url):\n",
    "    \"\"\"\n",
    "    주어진 URL에서 오디오 파일을 다운로드하여 넘파이 배열과 샘플링 레이트를 반환하는 함수.\n",
    "    \"\"\"\n",
    "    # URL에서 바이너리 데이터 가져오기\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # 요청 실패 시 예외 발생\n",
    "\n",
    "    # BytesIO로 감싸서 soundfile로 읽기\n",
    "    data, samplerate = sf.read(io.BytesIO(response.content), always_2d=True)\n",
    "    return data, samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e02e2-8807-4243-ab4f-79e163e8b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_audio_as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd72e0-ecc9-4ce3-a024-b0944408e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://hpxjdveijpuehyuykkos.supabase.co/storage/v1/object/public/user_uploads/2025-03-20/28c56a23-6c85-4b08-937a-7f8ef074abce/CC-DS%20Body%20Fall%20Concrete%20Soft%2002-glued.wav\"\n",
    "url = 'https://hpxjdveijpuehyuykkos.supabase.co/storage/v1/object/public/user_uploads/2025-03-31/788d9cf9-e013-4b39-8034-edae36b268f9/9e7d33cd-e33c-4e57-a81c-624cec6b4da3_pond_pond_0_low-8-bit-lo-fi-sound-effect-218319919_nw_prev.wav'\n",
    "url = \"https://hpxjdveijpuehyuykkos.supabase.co/storage/v1/object/public/user_uploads/2025-04-01/788d9cf9-e013-4b39-8034-edae36b268f9/c106633b-9878-4855-8179-ef8445d5e6c1_epidemic-audios_misc-musical_Bl8QyruCWW.wav\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # 요청 실패 시 예외 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efecf384-50ed-4ff9-9e0d-a5dad877f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = io.BytesIO(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b38b0-9a60-4ab3-bcba-3b113d9eeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, samplerate = sf.read(ad, always_2d=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae43b37-c03e-4c08-8ba2-ae07575e4417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd937e-9e05-4ec9-a4aa-b04336fba5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_urls(url_list):\n",
    "    \"\"\"\n",
    "    URL 리스트를 받아 int16 타입의 오디오 배열로 변환.\n",
    "    반환: [n, length, channels] 형태의 3D 배열\n",
    "    \"\"\"\n",
    "    audio_arrays = []\n",
    "    samplerates = []\n",
    "    max_length = 0\n",
    "\n",
    "    # 각 URL에서 오디오 다운로드 및 변환\n",
    "    for url in url_list:\n",
    "        data, samplerate = download_audio_as_array(url)\n",
    "\n",
    "        samplerates.append(samplerate)  \n",
    "        data_int16 = convert_to_int16(data)\n",
    "        \n",
    "        # 길이 업데이트\n",
    "        max_length = max(max_length, data_int16.shape[0])\n",
    "        audio_arrays.append(data_int16)\n",
    "\n",
    "    # 모든 오디오 데이터를 동일한 길이로 패딩\n",
    "    padded_audios = []\n",
    "    for audio in audio_arrays:\n",
    "        padding = ((0, max_length - audio.shape[0]), (0, 0))  # 시간축 패딩 추가\n",
    "        padded_audio = np.pad(audio, padding, mode='constant', constant_values=0)\n",
    "        padded_audios.append(padded_audio)\n",
    "\n",
    "    # [n, length, channels] 형태의 3D 배열로 병합\n",
    "    result = np.stack(padded_audios, axis=0)\n",
    "    return result, samplerates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501450b2-ecb3-4f19-b56b-a13efb7de54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = process_audio_urls([url, url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef95c1d-3c71-4876-ae6e-6cb6f6e6bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f3995-40fa-4268-af6c-190eb4b6a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166dbd53-155b-4241-87f9-2c51099709fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06ea03c-fb9f-410a-a842-a4ccef7f0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = [audio / np.iinfo(audio.dtype).max for audio in audios]\n",
    "audio_tensor = torch.from_numpy(np.stack(audios, axis=0)).to('cuda')\n",
    "audio_tensor = audio_tensor.float()\n",
    "##\n",
    "audio_tensor = audio_tensor.transpose(1, 2)\n",
    "audio_tensor = torchaudio.functional.resample(audio_tensor.contiguous(), orig_freq=96000, new_freq=44100)\n",
    "audio_tensor = audio_tensor.transpose(1, 2)\n",
    "if audio_tensor.shape[2] == 1:\n",
    "    audio_tensor = audio_tensor.repeat(1, 1, 2)\n",
    "elif audio_tensor.shape[2] > 2:\n",
    "    audio_tensor = audio_tensor[:, :, :2]\n",
    "target_len = audio_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885531c-5fb2-4f63-8f66-e712fabecb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617ec6b-5fa1-4011-83d3-e7ad04d2e121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0d30f-285a-40d4-be47-7e7fffb1ede9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
