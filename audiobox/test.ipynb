{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183338e3-f25e-44de-abdf-51fb26269883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import concurrent\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import soundfile as sf\n",
    "import io\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import repeat\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from vocos import get_voco\n",
    "from model.module import AudioBoxModule\n",
    "from torchode.interface import solve_ivp\n",
    "import torchaudio\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bf3b0-25dd-4810-bfdd-4aea5e8f551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "path = \"./new-stage-2.ckpt\"\n",
    "\n",
    "model = AudioBoxModule.load_from_checkpoint(path).to(device)\n",
    "model.eval()\n",
    "print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74649ffb-1dc7-4e25-8245-fdd15747cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "voco = get_voco('oobleck').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bba804-361d-46f6-949a-bb4d6cfb6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30d683-7a48-43e1-8c81-d52b2a9c0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "audio, sr = librosa.load('CC-DS Body Fall Concrete Soft 02-glued.wav', sr=44100, mono=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a8504-4917-4baa-9ec4-3cc4c5bb8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_len = voco.encode_length(44100*3)\n",
    "print(latent_len)\n",
    "print(voco.latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419d53c-ddcb-47af-8c32-84c6599958c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'body fall concrete soft'\n",
    "text_output = tokenizer(\n",
    "    [text + tokenizer.eos_token],\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=127,\n",
    "    truncation=\"longest_first\",\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "input_ids = text_output['input_ids'].to(device)\n",
    "attention_mask = text_output['attention_mask'].to(device).bool()\n",
    "\n",
    "print(\"input_ids : \", input_ids.shape)\n",
    "print(\"attention_mask : \", attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a4e57-f125-4030-9ae1-08165bbfbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embed = model.t5(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "print(text_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816dc9e1-8c56-471d-8212-a8e6663fd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "na = torch.from_numpy(audio).float().unsqueeze(dim=0)\n",
    "print(na.shape)\n",
    "na = rearrange(na, 'b c t -> b t c')\n",
    "print(na.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbfc89-c98f-4d66-880a-e022c7f5e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = voco.encode(na.to('cuda'))\n",
    "latent_len = latent.shape[1]\n",
    "max_latent_len = 400\n",
    "latent_mask = torch.arange(max_latent_len) < latent_len\n",
    "latent_mask = latent_mask.unsqueeze(dim=0)\n",
    "\n",
    "print(latent.shape)\n",
    "print(latent_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a6e26-9898-4f8c-8ad7-d52ff8a2d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = latent\n",
    "x0 = torch.randn_like(latent) # randn은 정규분포, rand는 유니폼 분포\n",
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf7971-5a3a-4d14-9743-0d0f7e548540",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = torch.rand((1,), dtype=latent.dtype, device=latent.device)\n",
    "print(times)\n",
    "\n",
    "time_step = rearrange(times, \"b -> b () ()\") # = unsqueeze 두번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e974ee-8b03-4479-9433-dcc9c5952d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1e-5\n",
    "\n",
    "xt = (1 - (1 - sigma) * time_step) * x0 + time_step * x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c2e91-985f-4f5e-9bfa-2ce63f30fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mask import min_span_mask, prob_mask_like\n",
    "\n",
    "span_mask = model.get_span_mask(latent_mask)\n",
    "print(span_mask.shape)\n",
    "\n",
    "cond_drop_mask = prob_mask_like((1, 1), model.drop_prob, model.device) # drop_prob 확률로 True\n",
    "audio_context_mask = span_mask | cond_drop_mask\n",
    "\n",
    "print(audio_context_mask.shape)\n",
    "\n",
    "print(latent_mask[0][:30])\n",
    "print(cond_drop_mask)\n",
    "print(span_mask[0][:30])\n",
    "print(audio_context_mask[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055928bd-68bc-4b61-9b88-2bbd85f70798",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_context = torch.where(\n",
    "    rearrange(audio_context_mask[:, :latent_len], \"b l -> b l ()\"), \n",
    "    0, \n",
    "    x1\n",
    ")\n",
    "print(rearrange(audio_context_mask[:, :latent_len], \"b l -> b l ()\").shape)\n",
    "print(audio_context.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0f6b0-7a20-414c-acc1-394e758d4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_drop_mask = prob_mask_like((1,), model.drop_prob, model.device)\n",
    "print(text_drop_mask)\n",
    "text_emb = torch.where(\n",
    "    rearrange(text_drop_mask, \"b -> b () ()\"), 0, text_embed\n",
    ")\n",
    "print(text_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ebcc87-8ae2-4e53-bef9-5207066f6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"w : \", xt.shape)\n",
    "print(\"time_step : \", time_step.shape)\n",
    "print(\"audio_mask : \", latent_mask.shape)\n",
    "print(\"audio_context : \", audio_context.shape)\n",
    "print(\"text_embed : \", text_embed.shape)\n",
    "print(\"attention_mask : \", attention_mask.shape)\n",
    "\n",
    "pred = model.audiobox(\n",
    "    w=xt,\n",
    "    times=time_step[0],\n",
    "    audio_mask=latent_mask.to(model.device),\n",
    "    context=audio_context,\n",
    "    phoneme_emb=text_embed,\n",
    "    phoneme_mask=attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5019a31-f735-414e-bbb0-88e933e040a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_flow = x1 - x0\n",
    "\n",
    "print(target_flow.shape)\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4df37f-be5b-4fcb-9aa6-8f34718395d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_latent_len = 215\n",
    "\n",
    "padded_latent = np.pad(latent.cpu()[0], ((0, max_latent_len - latent_len), (0, 0)))\n",
    "print(padded_latent.shape)\n",
    "\n",
    "padded_latent = torch.from_numpy(padded_latent).to(model.device)\n",
    "latent_mask = latent_mask.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe79c83-f2d4-498e-9588-e977855f46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b761a7-fa3d-4a04-aa18-7f446cc4916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.max_audio_len = 215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8334d00-3fe7-48a2-acd1-96d53a13ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\n",
    "    audio_enc=padded_latent.unsqueeze(dim=0),\n",
    "    audio_mask=latent_mask[:, :215],\n",
    "    phoneme=input_ids,\n",
    "    phoneme_mask=attention_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a334568-75e7-404e-82fb-bb341de9320e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239654f-dd3a-4f24-a37a-aeecb36d46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datas = [{\n",
    "    'audio_path': '/workspace/alignment-v3/audiobox/latent.npy',\n",
    "    'desc': \"test for cat\",\n",
    "    'duration': 1.5\n",
    "}]*10\n",
    "pd.DataFrame(datas).to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733477d-5d8e-4b8d-870f-f168faa33e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf43aab-f024-48f3-ade9-b62f629201a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data.dataset_0402 import AudioDataset\n",
    "\n",
    "datasets = AudioDataset(\n",
    "    dataset_path = './test.csv',\n",
    "    max_audio_len = 400,\n",
    "    sampling_rate = 44100,\n",
    "    max_txt_len = 127,\n",
    "    channel = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9e148-4856-42d1-af23-e45c8a078578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "dl = DataLoader(\n",
    "    datasets,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    collate_fn=datasets.collate,\n",
    "    prefetch_factor=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec00f94-3b5a-454e-9114-ff49b778a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3889f1-5fb2-456c-b1c6-b708f0dd913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent, latent_mask, text_input_ids, text_attention_mask = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228de3c-7911-428f-8ba9-3a60fe42fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\n",
    "    audio_enc=latent.to(model.device),\n",
    "    audio_mask=latent_mask.to(model.device),\n",
    "    phoneme=text_input_ids.to(model.device),\n",
    "    phoneme_mask=text_attention_mask.to(model.device),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560d628-f19b-4154-b8f2-52e330170404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "afs = os.listdir('./outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a016ed-7208-4e7c-801b-b4fae8f15849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "def analyze_spectrum_characteristics(file_path: str):\n",
    "    y, sr = librosa.load(file_path)\n",
    "\n",
    "    # Spectral Bandwidth\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
    "    bandwidth_mean = np.mean(bandwidth)\n",
    "\n",
    "    # Spectral Rolloff (e.g. 95%)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=0.95)[0]\n",
    "    rolloff_mean = np.mean(rolloff)\n",
    "\n",
    "    # Spectral Flatness\n",
    "    flatness = librosa.feature.spectral_flatness(y=y)[0]\n",
    "    flatness_mean = np.mean(flatness)\n",
    "\n",
    "    return {\n",
    "        \"bandwidth_mean\": bandwidth_mean,\n",
    "        \"rolloff_mean\": rolloff_mean,\n",
    "        \"flatness_mean\": flatness_mean\n",
    "    }\n",
    "\n",
    "def is_bright_sound(file_path: str, threshold: float = 3000.0) -> bool | None:\n",
    "    \"\"\"\n",
    "    Spectral centroid를 이용해 소리가 '밝은'지 판단합니다.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 오디오 파일 경로 (wav, mp3 등)\n",
    "        threshold (float): 밝음 판단 기준 (Hz). 보통 3000Hz 이상이면 밝은 소리로 판단.\n",
    "        \n",
    "    Returns:\n",
    "        True: 밝은 소리\n",
    "        False: 어두운 소리\n",
    "        None: 소리가 너무 짧거나 판단이 어려운 경우\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path)\n",
    "        \n",
    "        if len(y) < sr * 0.2:  # 0.2초보다 짧은 경우 판단 보류\n",
    "            return None\n",
    "\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        centroid_mean = np.mean(spectral_centroids)\n",
    "\n",
    "        if np.isnan(centroid_mean) or centroid_mean < 100:\n",
    "            return None  # 무의미한 값이면 판단하지 않음\n",
    "\n",
    "        return centroid_mean >= threshold\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "        return None\n",
    "\n",
    "total_bm = 0\n",
    "for i in tqdm(range(20)):\n",
    "    idx = random.randint(1, len(afs))\n",
    "    bm = analyze_spectrum_characteristics('./outputs/' + afs[idx])['bandwidth_mean']\n",
    "    total_bm += bm\n",
    "    print(bm)\n",
    "    AudioSignal('./outputs/' + afs[idx], duration=10.0).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0a467-d994-4e73-acfe-ff4c497e4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bm/2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71691a6-5091-4019-bfcd-18a4c2ed41f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
