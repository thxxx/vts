{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29979e41-0b7c-4ddb-9efe-fc8b7849402a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15facb91-371b-40a0-8865-53fcd86ece07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from collections import defaultdict\n",
    "\n",
    "latent_paths = os.listdir(\"/workspace/AVE_Datasetori/AVE_latents\")\n",
    "print(latent_paths[:5])\n",
    "print(os.listdir('/workspace/AVE_Datasetori/AVE_audio/')[0])\n",
    "\n",
    "with open('/workspace/AVE_Datasetori/Annotations.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()  # 리스트 형태로 모든 줄 읽음\n",
    "\n",
    "datas = []\n",
    "infos = defaultdict(list)\n",
    "\n",
    "print(lines[:5])\n",
    "for line in tqdm(lines):\n",
    "    d = line.split(\"&\")\n",
    "    infos[d[1]] = [d[0]]\n",
    "    # datas.append({\n",
    "    #     \"video_latent_path\":,\n",
    "    #     \"caption\":d[0],\n",
    "    #     \"duration\":\n",
    "    #     \"audio_path\":\n",
    "    # })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0901a3-3a75-413b-85d8-0dd767190f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "datas = []\n",
    "for latent_path in tqdm(latent_paths):\n",
    "    latent_id = latent_path[:-len(\"_126res_24fps.npy\")]\n",
    "    audio_path = f'/workspace/AVE_Datasetori/AVE_audio/{latent_id}_126res_24fps.wav'\n",
    "    caption = infos[latent_id]\n",
    "    audio, sr = librosa.load(audio_path)\n",
    "    duration = audio.shape[-1]/sr\n",
    "    video_latent_path = \"/workspace/AVE_Datasetori/AVE_latents/\" + latent_path\n",
    "    if not os.path.exists(video_latent_path):\n",
    "        print(\"stops!\", video_latent_path)\n",
    "    audio_latent_path = f'/workspace/AVE_Datasetori/AVE_audio_latent/{latent_id}_126res_24fps.npy'\n",
    "    if not os.path.exists(audio_latent_path):\n",
    "        print(\"stops!\", audio_latent_path)\n",
    "\n",
    "    datas.append({\n",
    "        \"video_latent_path\": video_latent_path,\n",
    "        \"caption\": caption[0],\n",
    "        \"duration\": duration,\n",
    "        \"audio_path\": audio_latent_path\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abe76e-bc7b-41c9-ac32-466e89f046e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c9629-283c-44a0-ad3e-a6aefaedbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('output_data.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['video_latent_path', 'caption', 'duration', 'audio_path'])\n",
    "    writer.writeheader()  # 컬럼명 쓰기\n",
    "    writer.writerows(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7b22b-c1f2-47f3-9a62-e535bc5c60b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82301c3-6e8f-4506-a519-d5223a57f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data.datamodule import AudioDataModule\n",
    "from lightning import Callback, LightningModule, Trainer, seed_everything\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "datamodule = AudioDataModule(\n",
    "    dataset_path='output_data.csv',\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    sampling_rate=44100,\n",
    "    channel=2,\n",
    "    max_audio_len=215,\n",
    "    max_txt_len=128,\n",
    "    max_video_len=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281fa5e-673e-4ab4-8f0e-e9de58546f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdbbe90-5701-4825-b2ea-39465e9177e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdt = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd7dec-1621-422d-a72c-4b3b9c4cd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(tdt))\n",
    "print(data[0])\n",
    "\n",
    "# %s does not exist.%s does not exist. %s does not exist./workspace/AVE_Datasetori/AVE_audio_latent/92sTzXubUgQ_126res_24fps.npy \n",
    "# /workspace/AVE_Datasetori/AVE_audio_latent/-j9TbKCJMwI_126res_24fps.npy \n",
    "# /workspace/AVE_Datasetori/AVE_audio_latent/92sTzXubUgQ_126res_24fps.npy does not exist.\n",
    "# %s does not exist.\n",
    "# /workspace/AVE_Datasetori/AVE_audio_latent/-h9REoRVzYY_126res_24fps.npy \n",
    "# /workspace/AVE_Datasetori/AVE_audio_latent/-j9TbKCJMwI_126res_24fps.npy does not exist.\n",
    "\n",
    "# %s does not exist./workspace/AVE_Datasetori/AVE_audio_latent/05ExLJ7xRis_126res_24fps.npy%s does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88d5d7-c8a9-4024-bcb6-7957c3b67eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714dd34-ffcc-4e2d-86fc-e3542a0df2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embed = data[0]\n",
    "video_latents = data[1]\n",
    "audio_mask = data[2]\n",
    "text = data[3]\n",
    "text_mask = data[4]\n",
    "\n",
    "print(audio_embed.shape)\n",
    "print(video_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84528aac-e39d-4584-a8a1-3732714b2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.audiobox_video import AudioBox\n",
    "from model.module_video import AudioBoxModule\n",
    "\n",
    "# pip uninstall torchvision\n",
    "# pip install torchvision --no-cache-dir\n",
    "\n",
    "audiobox = AudioBox(\n",
    "    audio_dim=64,\n",
    "    text_dim=768,\n",
    "    dim=1024,\n",
    "    depth=24,\n",
    "    heads=16,\n",
    "    attn_dropout=0.0,\n",
    "    ff_dropout=0.1,\n",
    "    kernel_size=31,\n",
    ")\n",
    "model = AudioBoxModule(\n",
    "    dim=64,\n",
    "    depth=24,\n",
    "    heads=16,\n",
    "    attn_dropout=0.0,\n",
    "    ff_dropout=0.1,\n",
    "    kernel_size=31,\n",
    "    voco_type=\"oobleck\",\n",
    "    optimizer=\"AdamW\",\n",
    "    lr=0.0001,\n",
    "    scheduler=\"linear_warmup_decay\",\n",
    "    max_audio_len=215,\n",
    "    max_steps=1000,\n",
    "    text_repo_id=\"google/flan-t5-base\",\n",
    ")\n",
    "device = 'cuda'\n",
    "audiobox.to(device)\n",
    "model.to(device)\n",
    "print(\"-\")\n",
    "\n",
    "import torch\n",
    "\n",
    "def count_parameters(mode):\n",
    "    return sum(p.numel() for p in mode.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {count_parameters(audiobox):,}\")\n",
    "# 433M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc1efa-dd64-43ef-a311-a7b0588ef59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mask import min_span_mask, prob_mask_like\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "audio_embed = audio_embed.to(device)\n",
    "text = text.to(device)\n",
    "text_mask = text_mask.to(device)\n",
    "video_latents = video_latents.to(device)\n",
    "audio_mask = audio_mask.to(device)\n",
    "\n",
    "bs = audio_embed.shape[0]\n",
    "with torch.no_grad():\n",
    "    span_mask = model.get_span_mask(audio_mask)\n",
    "    with torch.autocast(device_type=model.device.type, enabled=False):\n",
    "        text_emb = model.t5(\n",
    "            input_ids=text.to(device), attention_mask=text_mask.to(device)\n",
    "        ).last_hidden_state\n",
    "\n",
    "audio_x0 = torch.randn_like(audio_embed)\n",
    "times = torch.rand((bs,), dtype=audio_embed.dtype, device=model.device) # torch.rand는 0~1 uniform sampling\n",
    "t = rearrange(times, \"b -> b () ()\")\n",
    "w = (1 - (1 - model.sigma) * t) * audio_x0 + t * audio_embed\n",
    "\n",
    "cond_drop_mask = prob_mask_like((bs, 1), model.drop_prob, model.device)\n",
    "audio_cond_mask = span_mask | cond_drop_mask\n",
    "\n",
    "audio_context = torch.where(\n",
    "    rearrange(audio_cond_mask, \"b l -> b l ()\"), 0, audio_embed\n",
    ")\n",
    "\n",
    "text_drop_mask = prob_mask_like((bs,), model.drop_prob, model.device)\n",
    "text_emb = torch.where(\n",
    "    rearrange(text_drop_mask, \"b -> b () ()\"), 0, text_emb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd74931-3f0f-4a72-b162-53686485e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_audio_flow = audiobox(\n",
    "    w=w,\n",
    "    times=times,\n",
    "    audio_mask=audio_mask,\n",
    "    context=audio_context,\n",
    "    text_emb=text_emb,\n",
    "    text_mask=text_mask,\n",
    "    video_latent=video_latents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18429c65-9f61-4c9e-b9e0-f0f7863f44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_audio_flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3d3cc-a5ae-4e70-ad39-a181cded5866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af810fee-87cb-4820-9ddf-cc7ece147dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafcd15e-27f5-4107-9c1c-c8c6b84cdd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.randn((4, 30, 64))\n",
    "x = x.repeat_interleave(8, dim=1)  # [B, 240, 64]\n",
    "x = F.pad(x, pad=(0, 0, 0, 5))  # (C_left, C_right, T_left, T_right)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a409f3-1bc6-4af4-9a28-3170538a33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.pad(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c3f16-a17c-4a24-a979-6b9f5d1e3472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
