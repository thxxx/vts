{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040d603-d523-478e-803c-56235108f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import concurrent\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import soundfile as sf\n",
    "import io\n",
    "# optional\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import repeat\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from vocos import get_voco\n",
    "from model.module import AudioBoxModule\n",
    "from torchode.interface import solve_ivp\n",
    "import torchaudio\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d6670-a06f-40ac-93e7-a50f34ece7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model functions\n",
    "class Infer:\n",
    "    def __init__(self, path: Path):\n",
    "        self.device = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "        self.model = AudioBoxModule.load_from_checkpoint(path).to(self.device)\n",
    "        self.model.eval()\n",
    "        self.voco = get_voco(self.model.voco_type).to(self.device)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        self.steps = 64\n",
    "        self.alpha = 3.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    def encode_text(self, texts: list[str]) -> tuple[Tensor, Tensor]:\n",
    "        batch_encoding = self.tokenizer(\n",
    "            [text + self.tokenizer.eos_token for text in texts],\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=127,\n",
    "            truncation=\"longest_first\",\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        phoneme = batch_encoding.input_ids.to(self.device)\n",
    "        phoneme_mask = batch_encoding.attention_mask.to(self.device) > 0\n",
    "        phoneme_emb = self.model.t5(\n",
    "            input_ids=phoneme, attention_mask=phoneme_mask\n",
    "        ).last_hidden_state\n",
    "\n",
    "        return phoneme_emb, phoneme_mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @torch.autocast(device_type=\"cuda\")\n",
    "    def generate(\n",
    "        self, texts: list[str], dur: float, cfg=3.0\n",
    "    ) -> list[np.ndarray]:\n",
    "        phoneme_emb, phoneme_mask = self.encode_text(texts)\n",
    "        batch_size = phoneme_emb.shape[0]\n",
    "\n",
    "        target_len = round(self.model.sampling_rate * dur)\n",
    "        latent_len = self.voco.encode_length(target_len)\n",
    "        audio_mask = torch.ones(\n",
    "            batch_size, latent_len, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "        audio_context = torch.zeros(\n",
    "            batch_size, latent_len, self.voco.latent_dim, device=self.device\n",
    "        )\n",
    "\n",
    "        if latent_len < 192:\n",
    "            audio_mask = F.pad(audio_mask, (0, 192 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 192 - latent_len))\n",
    "        elif 192 < latent_len < 384:\n",
    "            audio_mask = F.pad(audio_mask, (0, 384 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 384 - latent_len))\n",
    "        elif 384 < latent_len < 768:\n",
    "            audio_mask = F.pad(audio_mask, (0, 768 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 768 - latent_len))\n",
    "        elif 768 < latent_len < 1536:\n",
    "            audio_mask = F.pad(audio_mask, (0, 1536 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 1536 - latent_len))\n",
    "\n",
    "        def fn(t: Tensor, y: Tensor):\n",
    "            out = self.model.audiobox.cfg(\n",
    "                w=y,\n",
    "                context=audio_context,\n",
    "                times=t,\n",
    "                alpha=cfg,\n",
    "                mask=audio_mask,\n",
    "                phoneme_emb=phoneme_emb,\n",
    "                phoneme_mask=phoneme_mask\n",
    "            )\n",
    "            return out\n",
    "\n",
    "        y0 = torch.randn_like(audio_context)\n",
    "        t = torch.linspace(0, 1, self.steps, device=self.device)\n",
    "\n",
    "        t = repeat(t, \"n -> b n\", b=batch_size)\n",
    "        sol = solve_ivp(\n",
    "            fn,\n",
    "            # torch.compile(fn, dynamic=False),\n",
    "            y0,\n",
    "            t,\n",
    "            method_class=self.model.method, #self.model.torchode_method_klass,\n",
    "        )\n",
    "        sampled_audio = sol.ys[-1]\n",
    "\n",
    "        sample = self.voco.decode(sampled_audio)\n",
    "        sample = sample[:, :target_len]\n",
    "\n",
    "        sample = sample / sample.abs().amax(dim=1, keepdim=True).clamp_min(1)\n",
    "        sample = sample.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "        return [audio for audio in sample]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @torch.autocast(device_type=\"cuda\")\n",
    "    def variation(\n",
    "        self, audios: list[np.ndarray], texts: list[str], dur: float, corrupt: float, sr: list[int], cfg_score: int = 3.0, voice_enc: Tensor=None\n",
    "    ) -> list[np.ndarray]:\n",
    "        phoneme_emb, phoneme_mask = self.encode_text(texts)\n",
    "        batch_size = phoneme_emb.shape[0]\n",
    "\n",
    "        audios = [audio / np.iinfo(audio.dtype).max for audio in audios]\n",
    "        audio_tensor = torch.from_numpy(np.stack(audios, axis=0)).to(self.device)\n",
    "        audio_tensor = audio_tensor.float()\n",
    "        ##\n",
    "        audio_tensor = audio_tensor.transpose(1, 2)\n",
    "        audio_tensor = torchaudio.functional.resample(audio_tensor, orig_freq=sr[0], new_freq=self.voco.sampling_rate)\n",
    "        audio_tensor = audio_tensor.transpose(1, 2)\n",
    "        if audio_tensor.shape[2] == 1:\n",
    "            audio_tensor = audio_tensor.repeat(1, 1, 2)\n",
    "        elif audio_tensor.shape[2] > 2:\n",
    "            audio_tensor = audio_tensor[:, :, :2]\n",
    "        target_len = audio_tensor.shape[1]\n",
    "        latent_len = self.voco.encode_length(target_len)\n",
    "        audio_enc = self.voco.encode(audio_tensor)\n",
    "        audio_mask = torch.ones(\n",
    "            batch_size, latent_len, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "        audio_context = torch.zeros(\n",
    "            batch_size, latent_len, self.voco.latent_dim, device=self.device\n",
    "        )\n",
    "\n",
    "        if latent_len < 192:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 192 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 192 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 192 - latent_len))\n",
    "        elif 192 < latent_len < 384:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 384 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 384 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 384 - latent_len))\n",
    "        elif 384 < latent_len < 768:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 768 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 768 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 768 - latent_len))\n",
    "        elif 768 < latent_len < 1536:\n",
    "            audio_enc = F.pad(audio_enc, (0, 0, 0, 1536 - latent_len))\n",
    "            audio_mask = F.pad(audio_mask, (0, 1536 - latent_len))\n",
    "            audio_context = F.pad(audio_context, (0, 0, 0, 1536 - latent_len))\n",
    "\n",
    "        sigma = 1e-3\n",
    "        c = 1.0 - corrupt\n",
    "        noised_enc = (audio_enc * c) + torch.randn_like(audio_enc) * (1 - (1 - sigma) * c)\n",
    "        corrupt_tensor = torch.tensor(1 - corrupt).to(self.device)\n",
    "        # print(\"corrupt_tensor : \", corrupt_tensor)\n",
    "\n",
    "        def forward(t: Tensor, y: Tensor):\n",
    "            # print(\"times : \", t)\n",
    "            out = self.model.audiobox.cfg(\n",
    "                w=y,\n",
    "                context=audio_context,\n",
    "                # times=t,\n",
    "                times=t,\n",
    "                alpha=cfg_score,\n",
    "                mask=audio_mask,\n",
    "                phoneme_emb=phoneme_emb,\n",
    "                phoneme_mask=phoneme_mask\n",
    "            )\n",
    "            return out\n",
    "\n",
    "        # t = torch.linspace(c, 1, 64, device=self.device)\n",
    "        t = torch.linspace(0, corrupt, self.steps, device=self.device)\n",
    "\n",
    "        t = repeat(t, \"n -> b n\", b=batch_size)\n",
    "        sol = solve_ivp(\n",
    "            # torch.compile(forward, dynamic=False),\n",
    "            forward,\n",
    "            noised_enc,\n",
    "            t+corrupt_tensor, # 0.6 ~ 1.0\n",
    "            method_class=self.model.method #.torchode_method_klass,\n",
    "        )\n",
    "        sampled_audio = sol.ys[-1]\n",
    "\n",
    "        sample = self.voco.decode(sampled_audio)\n",
    "        new_target_len = round(self.model.sampling_rate * dur)\n",
    "        sample = sample[:, :new_target_len]\n",
    "\n",
    "        sample = sample / sample.abs().amax(dim=1, keepdim=True).clamp_min(1)\n",
    "        sample = sample.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "        return [audio for audio in sample]\n",
    "\n",
    "def convert_to_int16(audio_array):\n",
    "    \"\"\"\n",
    "    오디오 배열을 int16 형식으로 변환.\n",
    "    \"\"\"\n",
    "    # float형 오디오 배열을 int16 범위로 스케일링\n",
    "    audio_array = np.clip(audio_array, -1.0, 1.0)  # -1.0 ~ 1.0 범위로 제한\n",
    "    audio_int16 = (audio_array * 32767).astype(np.int16)\n",
    "    return audio_int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e02e3-30be-4a5b-8150-950faee1a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "infer = Infer(Path(\"./text_alignment_v3_0414_65000.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3d92e-fe8c-443f-8942-acfc34ba1d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09698d7-1490-4445-a8c0-2c6b31e951eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from audiotools import AudioSignal\n",
    "import time\n",
    "import torchaudio\n",
    "\n",
    "st = time.time()\n",
    "output_audios = infer.generate([\n",
    "    \"plane plummeted to the ground after losing power and explosiong.\"\n",
    "]*5, 6.0, 3.0)\n",
    "print(time.time() - st)\n",
    "\n",
    "for o in output_audios:\n",
    "    AudioSignal(rearrange(o, 't s -> s t'), sample_rate=44100).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2047c4e-d545-4206-99e5-9fd9a210d4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58c6cc-3631-47a8-aae0-4ccc4986e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from audiotools import AudioSignal\n",
    "import time\n",
    "import torchaudio\n",
    "\n",
    "st = time.time()\n",
    "output_audios = infer.generate([\n",
    "    \"plane plummeted to the ground after losing power and explosiong.\"\n",
    "]*5, 6.0, 3.0)\n",
    "print(time.time() - st)\n",
    "\n",
    "for o in output_audios:\n",
    "    AudioSignal(rearrange(o, 't s -> s t'), sample_rate=44100).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f74bfa-5ed4-4226-bcdc-ebdcdce52b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046f8a8-6e07-42c8-baf9-166106ffbb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e67fef-44a1-4346-8df8-d00e68154130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50affff0-fcb1-4891-a59e-d165d321c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from audiotools import AudioSignal\n",
    "import time\n",
    "import torchaudio\n",
    "from utils.utils import get_dynamic\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def span_mask(tensor, mask_frac=0.3, min_len=4, max_len=10):\n",
    "    B, T, D = tensor.shape\n",
    "    assert B == 1, \"이 코드는 배치 크기 1 기준입니다.\"\n",
    "\n",
    "    total_mask_len = int(T * mask_frac)\n",
    "    mask = torch.zeros(T, dtype=torch.bool)\n",
    "\n",
    "    masked = 0\n",
    "    while masked < total_mask_len:\n",
    "        span_len = random.randint(min_len, max_len)\n",
    "        if masked + span_len > total_mask_len:\n",
    "            span_len = total_mask_len - masked\n",
    "\n",
    "        start = random.randint(0, T - span_len)\n",
    "        mask[start : start + span_len] = True\n",
    "        masked += span_len\n",
    "\n",
    "    # [1, 100, 12] shape에 맞게 broadcast\n",
    "    mask = mask[None, :, None].expand(B, T, D)  # [1, 100, 12]\n",
    "\n",
    "    # 마스킹 적용\n",
    "    masked_tensor = tensor.masked_fill(mask, 0.0)\n",
    "    return masked_tensor\n",
    "\n",
    "def span_mask_strided(tensor, span_len=6, stride=16):\n",
    "    \"\"\"\n",
    "    일정한 간격(stride)마다 일정 길이(span_len)로 마스크를 적용하는 함수\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor): 입력 텐서, shape (1, T, D)\n",
    "        span_len (int): 마스크될 구간 길이\n",
    "        stride (int): 마스크가 시작되는 위치 간격\n",
    "\n",
    "    Returns:\n",
    "        masked_tensor: 마스킹이 적용된 텐서\n",
    "    \"\"\"\n",
    "    B, T, D = tensor.shape\n",
    "    assert B == 1, \"이 코드는 배치 크기 1 기준입니다.\"\n",
    "\n",
    "    mask = torch.zeros(T, dtype=torch.bool)\n",
    "    for start in range(0, T, stride):\n",
    "        end = min(start + span_len, T)\n",
    "        mask[start:end] = True\n",
    "\n",
    "    # [1, T, D]로 broadcast\n",
    "    mask = mask[None, :, None].expand(B, T, D)\n",
    "    masked_tensor = tensor.masked_fill(mask, 0.0)\n",
    "    return masked_tensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524cb328-64ce-485d-8031-2d185b3b9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofile = './voice_samples/charging.m4a'\n",
    "sampling_rate = 44100\n",
    "\n",
    "waveform, sr = torchaudio.load(audiofile)\n",
    "if sr != sampling_rate:\n",
    "    waveform = torchaudio.functional.resample(waveform, sr, sampling_rate)\n",
    "waveform = waveform.mean(dim=0, keepdim=True)[:, :int(44100*5)]\n",
    "\n",
    "dynamic = get_dynamic(waveform, max_len=110)\n",
    "\n",
    "print(waveform.shape)\n",
    "print(dynamic.shape)\n",
    "\n",
    "voice_enc=dynamic.unsqueeze(dim=0)\n",
    "\n",
    "# voice_enc_masked = span_mask(voice_enc, 0.0, 4, 10)\n",
    "# visualize_mask(voice_enc, voice_enc_masked)\n",
    "\n",
    "voice_enc_masked = span_mask_strided(voice_enc, 1, 2)\n",
    "# voice_enc_masked[:, :, :4] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4ac15-19a6-431d-b76a-51e44733cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 시각화할 feature index들\n",
    "feature_indices = [0, 4, 8]\n",
    "\n",
    "# Plot\n",
    "for i, idx in enumerate(feature_indices):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    data = voice_enc_masked[0, :, idx]  # shape: (100,)\n",
    "    plt.plot(data)\n",
    "    plt.title(f'Feature Index {idx}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143c439-ec94-4667-beac-780314528205",
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSignal(waveform, sample_rate=44100).widget()\n",
    "\n",
    "outputs = []\n",
    "for _ in range(3):\n",
    "    output_audios = infer.generate([\n",
    "        \"Sound of sci-fi cannong charging and shooting. & rich pitch range in sci-fi style.\"\n",
    "        # \"rocket launching and explosion.\"\n",
    "        # \"Shotgun reloading sound. & low pitch range\"\n",
    "        # \"Sound of coin dropping on iron surface. & low pitch range\"\n",
    "    ]*1, 5.0, 3.0, voice_enc=voice_enc_masked.to('cuda'))\n",
    "    for aa in output_audios:\n",
    "        aa = rearrange(aa, 's t -> t s')\n",
    "        AudioSignal(aa, sample_rate=44100).widget()\n",
    "        outputs.append(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885531c-5fb2-4f63-8f66-e712fabecb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(\"./cannon3.wav\", torch.tensor(outputs[2]), sample_rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617ec6b-5fa1-4011-83d3-e7ad04d2e121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f39bf4-d083-4761-a5fc-e35a6164c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofile = \"./voice_samples/tiriring.m4a\"\n",
    "duration = 3.0\n",
    "prompt =  \"level up UI sound in game.\" # \"Sound of coin dropping on iron surface.\"\n",
    "save_word = \"tiriring\"\n",
    "\n",
    "sampling_rate = 44100\n",
    "waveform, sr = torchaudio.load(audiofile)\n",
    "if sr != sampling_rate:\n",
    "    waveform = torchaudio.functional.resample(waveform, sr, sampling_rate)\n",
    "waveform = waveform.mean(dim=0, keepdim=True)[:, int(44100*0.0):int(44100*3.0)]\n",
    "\n",
    "dynamic = get_dynamic(waveform, max_len=110)\n",
    "voice_enc=dynamic.unsqueeze(dim=0)\n",
    "voice_enc_masked = span_mask_strided(voice_enc, 1, 2)\n",
    "# voice_enc_masked[:, :, :4] = 0.0\n",
    "\n",
    "for i in range(3):\n",
    "    output_audios = infer.generate([\n",
    "        prompt,\n",
    "        # \"rocket launching and explosion.\"\n",
    "        # \"Shotgun reloading sound. & low pitch range\"\n",
    "        # \"Sound of coin dropping on iron surface. & low pitch range\"\n",
    "    ]*1, duration, 3.0, voice_enc=voice_enc_masked.to('cuda'))\n",
    "    for aa in output_audios:\n",
    "        aa = rearrange(aa, 's t -> t s')\n",
    "        AudioSignal(aa, sample_rate=44100).widget()\n",
    "        torchaudio.save(f\"./{save_word}_{i}.wav\", torch.tensor(aa), sample_rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082463a1-ba2c-48da-a31d-6437455802e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0d30f-285a-40d4-be47-7e7fffb1ede9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873c013-f118-4dda-a6e1-18ff8a94e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_audios = convert_to_int16(np.array(rearrange(outputs[0], 't s -> () t s')))\n",
    "print(merged_audios.shape)\n",
    "\n",
    "duration = 5.0\n",
    "temperature = 0.5\n",
    "texts = \"Sound of sci-fi cannong charging and shooting. & rich pitch range in sci-fi style.\"\n",
    "voice_enc_zeros = torch.zeros_like(voice_enc).to('cuda')\n",
    "\n",
    "gen = infer.variation(merged_audios, [texts] * 1, duration, temperature, [44100], cfg_score=3.0, voice_enc=voice_enc_zeros)\n",
    "gen = rearrange(gen[0], 't s -> s t')\n",
    "AudioSignal(gen, sample_rate=44100).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c3f8b-6a7a-48bf-b119-cc34fd9d4148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbf127-ef9a-4f34-ad7e-9f4b617b7b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef5ac4-7351-44ee-b62c-5be303ec1b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
