{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448b464-9f01-4d11-9288-69348ca40dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "# ltxv_model_path = hf_hub_download(\n",
    "#     repo_id=\"Lightricks/LTX-Video\",\n",
    "#     filename=\"./ltxv-2b-0.9.8-distilled.safetensors\",\n",
    "#     repo_type=\"model\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c89f6e-196c-41b5-8c24-08b079ee240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltx_video.models.autoencoders.causal_video_autoencoder import CausalVideoAutoencoder\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "ltxv_model_path = './ltxv-2b-0.9.8-distilled.safetensors'\n",
    "vae = CausalVideoAutoencoder.from_pretrained(ltxv_model_path)\n",
    "vae.to(device)\n",
    "print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4141c6-708e-4eb0-9d19-17a740938e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_video\n",
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "vid_path = '/workspace/sixteen128.mp4'\n",
    "\n",
    "video_frames, audio, info = read_video(vid_path)\n",
    "print(\"info : \", info)\n",
    "\n",
    "print(video_frames.shape, video_frames.dtype)  # (T, H, W, C)  # T: frame count, C=3\n",
    "video_frames = rearrange(video_frames.unsqueeze(dim=0).tile(2, 1, 1, 1, 1), 'b t h w c -> b c t h w').to(device).to(torch.float32)\n",
    "video_frames = video_frames[:, :, :65, :, :] # 8의 배수 + 1이어야 한다.\n",
    "print(video_frames.shape, video_frames[0][0][0][0])\n",
    "\n",
    "video_frames = (video_frames/255) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683cfa29-8f91-46f6-b6a6-f42e5ebde4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    aeoutput = vae.encode(video_frames)\n",
    "    latent = aeoutput.latent_dist.mode()\n",
    "    print(latent.shape)\n",
    "\n",
    "del aeoutput\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a53be-5501-4145-b12e-681d18fd879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "fss = os.listdir(\"/workspace/AVE_Dataset/AVE_latents/\")\n",
    "print(fss[0])\n",
    "\n",
    "npz = np.load(\"/workspace/AVE_Dataset/AVE_latents/\" + fss[0])\n",
    "print(npz.shape)\n",
    "tt = torch.tensor(npz).unsqueeze(dim=0).to(device)\n",
    "print(tt.shape)\n",
    "\n",
    "timestep = torch.ones(1, device=device) * 0.1\n",
    "ts = torch.randn(((1, 3, 240, 128, 128)))\n",
    "\n",
    "reconstructed_videos = vae.decode(\n",
    "    tt, target_shape=tt.shape, timestep=timestep\n",
    ").sample\n",
    "\n",
    "print(reconstructed_videos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c503f-b247-4187-8ffe-16cc9ab7c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = torch.ones(video_frames.shape[0], device=device) * 0.1\n",
    "\n",
    "reconstructed_videos = vae.decode(\n",
    "    latent[:, :, :4, :, :], target_shape=video_frames[:, :, :49, :, :].shape, timestep=timestep\n",
    ").sample\n",
    "\n",
    "print(reconstructed_videos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e9fdb-7514-4ae1-8f02-2b325a4f84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.io import read_video, write_video\n",
    "from einops import rearrange\n",
    "\n",
    "output_path = './reencoded_video12816test_2.mp4'\n",
    "recon_videos = (rearrange(reconstructed_videos, \"b c t h w -> b t h w c\")[0].cpu()/2 + 0.5) * 255\n",
    "\n",
    "write_video(\n",
    "    filename=output_path,\n",
    "    video_array=recon_videos,      # shape: (T, H, W, C)\n",
    "    fps=int(24),\n",
    "    video_codec='libx264',         # optional\n",
    "    options={\"crf\": \"18\"}          # optional: quality setting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b695c1-13c2-4c00-b1a4-c378c41baf45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95350f-b998-4895-bf54-f7a525d6ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_videos[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228f79b-8fc9-488b-8793-b9ec16b3b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f36a61-a9c3-4705-bc1e-b958e3673f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# spatial 4×4를 1×1로 줄이기 위한 3D‑Conv\n",
    "#   kernel_size=(1,4,4), stride=(1,4,4)\n",
    "conv3d = nn.Conv3d(\n",
    "    in_channels=128,\n",
    "    out_channels=128,\n",
    "    kernel_size=(1, 4, 4),\n",
    "    stride=(1, 4, 4),\n",
    "    padding=0,\n",
    ").to(device)\n",
    "\n",
    "y = conv3d(latent).squeeze()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a719040-c9ae-470d-b550-b13b08072fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57dec44-d714-4bc7-a1de-ce768591be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRN(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Gx = torch.norm(x, p=2, dim=1, keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        return self.gamma * (x * Nx) + self.beta + x\n",
    "\n",
    "class ConvNeXtV2Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        intermediate_dim: int,\n",
    "        dilation: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        padding = (dilation * (7 - 1)) // 2\n",
    "        self.dwconv = nn.Conv1d(\n",
    "            dim, dim, kernel_size=7, padding=padding, groups=dim, dilation=dilation\n",
    "        )  # depthwise conv\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, intermediate_dim)  # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.grn = GRN(intermediate_dim)\n",
    "        self.pwconv2 = nn.Linear(intermediate_dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input x is sequence, tensor of (bs, seq_len, dim)\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = x.transpose(1, 2)  # b n d -> b d n\n",
    "        x = self.dwconv(x)\n",
    "        x = x.transpose(1, 2)  # b d n -> b n d\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.pwconv2(x)\n",
    "        return residual + x\n",
    "\n",
    "cnv = ConvNeXtV2Block(128, 64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749a6a4-0fc7-4b6a-8f6d-3d4e6b6a131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn((2, 13, 32))\n",
    "out = cnv(rearrange(y, 'b d n -> b n d'))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2cd1a9-c91c-4701-a67e-a504230faf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class UpsampleHalveChannel(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose1d(\n",
    "            in_channels=in_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(out_ch)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.up(x)                   # → [B, C/2, 2L]\n",
    "        x = x.transpose(1, 2)            # → [B, 2L, C/2]\n",
    "        x = self.norm(x)\n",
    "        return self.act(x)\n",
    "\n",
    "upconv = UpsampleHalveChannel(128, 64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe15b3-dddd-4c27-8eff-576e6738deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "upconv(rearrange(y, 'b d n -> b n d')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2acf9e-4faa-4ad7-9b20-8ef1ec32b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Linear(128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc34dd-8c0e-4006-8895-fbaa6abf9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in conv3d.parameters())\n",
    "print(f\"conv3d parameters: {total_params:,}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in cnv.parameters())\n",
    "print(f\"cnv parameters: {total_params*4:,}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in layers.parameters())\n",
    "print(f\"layers parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b453a4-0d4d-4d49-acfa-5bd36ccc1aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
